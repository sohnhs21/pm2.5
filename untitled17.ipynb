{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwclIZk6BlvJF/0eLazWUM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sohnhs21/pm2.5/blob/main/untitled17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ougBYnqeUoOP"
      },
      "outputs": [],
      "source": [
        "pip install numpy pandas tensorflow keras sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "one model"
      ],
      "metadata": {
        "id": "ier4sQd_U3pY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "\n",
        "np.random.seed(46)\n",
        "tf.random.set_seed(46)\n",
        "\n",
        "def has_missing_values(array):\n",
        "    return np.isnan(array).any()\n",
        "\n",
        "def prepare_data(data, lookback, future):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data) - lookback - future):\n",
        "        X.append(data[i : (i + lookback), :])\n",
        "        Y.append(data[(i + lookback) : (i + lookback + future), 0])  \n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "def prepare_data_with_missing(data, lookback, future):\n",
        "    X, Y, missing_indices = [], [], []\n",
        "    for i in range(len(data) - lookback - future):\n",
        "        input_sequence = data[i : (i + lookback), :]\n",
        "        output_sequence = data[(i + lookback) : (i + lookback + future), 0]\n",
        "\n",
        "        if has_missing_values(input_sequence):\n",
        "            continue\n",
        "\n",
        "        X.append(input_sequence)\n",
        "\n",
        "        if has_missing_values(output_sequence):\n",
        "            missing_indices.append(i)\n",
        "        else:\n",
        "            Y.append(output_sequence)\n",
        "\n",
        "    return np.array(X), np.array(Y), missing_indices\n",
        "\n",
        "lookback = 2 * 24  # 2 days of hourly data\n",
        "future = 3 * 24    # 3 days of hourly data\n",
        "n_features = 6     # Number of features in the data\n",
        "\n",
        "locations = ['공주', '노은동', '논산', '대천2동', '독곶리', '동문동', '모종동', '문창동', '성성동', '신방동', '신흥동', '아름동', '예산군', '읍내동', '이원면', '정림동', '홍성읍']  # List all 17 locations\n",
        "\n",
        "X_train_all = np.empty((0, lookback, n_features))\n",
        "Y_train_all = np.empty((0, future))\n",
        "scalers = {}\n",
        "\n",
        "for location in locations:\n",
        "    train_data_loc = sorted_data[sorted_data['지점'] == location]\n",
        "    \n",
        "    # Drop the '지점' and '관측시점' columns\n",
        "    train_data_loc = train_data_loc.drop(['지점', '관측시점'], axis=1).values\n",
        "    \n",
        "    # Scale the data\n",
        "    scaler = MinMaxScaler()\n",
        "    train_data_loc = scaler.fit_transform(train_data_loc)\n",
        "    scalers[location] = scaler\n",
        "\n",
        "    # Prepare the data for LSTM\n",
        "    X_train, Y_train = prepare_data(train_data_loc, lookback, future)\n",
        "    \n",
        "    X_train_all = np.vstack((X_train_all, X_train))\n",
        "    Y_train_all = np.vstack((Y_train_all, Y_train))\n",
        "\n",
        "# Create the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, activation='tanh', input_shape=(lookback, n_features), return_sequences=True))\n",
        "model.add(Dropout(0.2))  # Add dropout with a dropout rate of 0.2\n",
        "model.add(LSTM(256, activation='tanh'))\n",
        "model.add(Dropout(0.2))  # Add dropout\n",
        "model.add(Dense(future))\n",
        "model.compile(optimizer='adam', loss='mae')\n",
        "\n",
        "# Add EarlyStopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model with validation split\n",
        "history = model.fit(X_train_all, Y_train_all, epochs=30, verbose=0, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Get the train and validation losses\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Make predictions\n",
        "Y_train_pred = model.predict(X_train_all)\n",
        "Y_val_pred = model.predict(X_train_all[-int(0.2 * len(X_train_all)):])\n",
        "\n",
        "# Calculate the mean absolute error\n",
        "train_mae = mean_absolute_error(Y_train_all, Y_train_pred)\n",
        "val_mae = mean_absolute_error(Y_train_all[-int(0.2 * len(Y_train_all)):], Y_val_pred)\n",
        "\n",
        "print(f\"Train MAE: {train_mae}, Validation MAE: {val_mae}\")\n",
        "\n",
        "all_test_data_real = pd.DataFrame()\n",
        "\n",
        "for location in locations:\n",
        "    test_data_loc = sort_data[sort_data['지점'] == location]\n",
        "    \n",
        "    # Drop the '지점' and '관측시점' columns\n",
        "    test_data_loc = test_data_loc.drop(['지점', '관측시점'], axis=1).values\n",
        "    \n",
        "    # Scale the data\n",
        "    scaler = scalers[location]\n",
        "    test_data_loc = scaler.transform(test_data_loc)\n",
        "\n",
        "    # Prepare the data for LSTM\n",
        "    X_test, Y_test, missing_indices = prepare_data_with_missing(test_data_loc, lookback, future)\n",
        "\n",
        "    Y_test_missing_pred = model.predict(X_test)\n",
        "\n",
        "    for i, missing_index in enumerate(missing_indices):\n",
        "        start = missing_index + lookback\n",
        "        end = start + future\n",
        "        test_data_loc[start:end, 0] = Y_test_missing_pred[i]\n",
        "\n",
        "    # Inverse transform the test data to get real values\n",
        "    test_data_loc_real = scaler.inverse_transform(test_data_loc)\n",
        "\n",
        "    # Create a new DataFrame with real values\n",
        "    test_data_loc_real_df = pd.DataFrame(test_data_loc_real, columns=['PM2.5', 'tem', 'wind degree', 'wind velocity', 'precipitation', 'humidity'])\n",
        "\n",
        "    # Get the '지점' and '관측시점' columns from the original test data\n",
        "    test_data_loc_observation = sort_data.loc[sort_data['지점'] == location, ['지점', '관측시점']].reset_index(drop=True)\n",
        "\n",
        "    # Concatenate '지점' and '관측시점' columns with the test_data_loc_real_df DataFrame\n",
        "    test_data_loc_real_df = pd.concat([test_data_loc_observation, test_data_loc_real_df], axis=1)\n",
        "\n",
        "    # Append the test_data_loc_real_df to the all_test_data_real DataFrame\n",
        "    all_test_data_real = all_test_data_real.append(test_data_loc_real_df, ignore_index=True)    \n",
        "\n",
        "# Sort the DataFrame by '지점' and '관측시점'\n",
        "all_test_data_real = all_test_data_real.sort_values(by=['지점', '관측시점']).reset_index(drop=True)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "all_test_data_real.to_csv('/content/new/result_46.csv', index=False)"
      ],
      "metadata": {
        "id": "KEOXczaFUwWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "one model and one hot incoding"
      ],
      "metadata": {
        "id": "bCLBDi4xVD1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "\n",
        "np.random.seed(47)\n",
        "tf.random.set_seed(47)\n",
        "\n",
        "def has_missing_values(array):\n",
        "    return np.isnan(array).any()\n",
        "\n",
        "def prepare_data(data, lookback, future):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data) - lookback - future):\n",
        "        X.append(data[i : (i + lookback), :])\n",
        "        Y.append(data[(i + lookback) : (i + lookback + future), 0])  \n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "def prepare_data_with_missing(data, lookback, future):\n",
        "    X, Y, missing_indices = [], [], []\n",
        "    for i in range(len(data) - lookback - future):\n",
        "        input_sequence = data[i : (i + lookback), :]\n",
        "        output_sequence = data[(i + lookback) : (i + lookback + future), 0]\n",
        "\n",
        "        if has_missing_values(input_sequence) or has_missing_values(output_sequence):\n",
        "            continue\n",
        "\n",
        "        X.append(input_sequence)\n",
        "        Y.append(output_sequence)\n",
        "\n",
        "        if np.isnan(output_sequence).any():\n",
        "            missing_indices.append(i)\n",
        "\n",
        "    return np.array(X), np.array(Y), missing_indices\n",
        "\n",
        "lookback = 2 * 24  # 2 days of hourly data\n",
        "future = 3 * 24    # 3 days of hourly data\n",
        "n_features = 6 + 17  # Number of features in the data\n",
        "\n",
        "locations = ['공주', '노은동', '논산', '대천2동', '독곶리', '동문동', '모종동', '문창동', '성성동', '신방동', '신흥동', '아름동', '예산군', '읍내동', '이원면', '정림동', '홍성읍']  # List all 17 locations\n",
        "\n",
        "X_train_all = np.empty((0, lookback, n_features))\n",
        "Y_train_all = np.empty((0, future))\n",
        "scalers = {}\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded_locations = encoder.fit_transform(sorted_data['지점'].values.reshape(-1, 1))\n",
        "encoded_test_locations = encoder.transform(sort_data['지점'].values.reshape(-1, 1))\n",
        "\n",
        "for location in locations:\n",
        "    train_data_loc = sorted_data[sorted_data['지점'] == location]\n",
        "    \n",
        "    # Drop the '지점' and '관측시점' columns\n",
        "    train_data_loc = train_data_loc.drop(['지점', '관측시점'], axis=1).values\n",
        "    \n",
        "    # Scale the data\n",
        "    scaler = MinMaxScaler()\n",
        "    train_data_loc = scaler.fit_transform(train_data_loc)\n",
        "    scalers[location] = scaler\n",
        "\n",
        "    # Add one-hot encoded location to the train_data_loc\n",
        "    location_encoded = encoded_locations[sorted_data['지점'] == location]\n",
        "    train_data_loc = np.hstack((location_encoded, train_data_loc))\n",
        "\n",
        "    # Prepare the data for LSTM\n",
        "    X_train, Y_train = prepare_data(train_data_loc, lookback, future)\n",
        "    \n",
        "    X_train_all = np.vstack((X_train_all, X_train))\n",
        "    Y_train_all = np.vstack((Y_train_all, Y_train))\n",
        "\n",
        "# Create the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, activation='tanh', input_shape=(lookback, n_features), return_sequences=True))\n",
        "model.add(Dropout(0.2))  # Add dropout with a dropout rate of 0.2\n",
        "model.add(LSTM(256, activation='tanh'))\n",
        "model.add(Dropout(0.2))  # Add dropout\n",
        "model.add(Dense(future))\n",
        "model.compile(optimizer='adam', loss='mae')\n",
        "\n",
        "# Add EarlyStopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model with validation split\n",
        "history = model.fit(X_train_all, Y_train_all, epochs=50, verbose=0, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Get the train and validation losses\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Make predictions\n",
        "Y_train_pred = model.predict(X_train_all)\n",
        "Y_val_pred = model.predict(X_train_all[-int(0.2 * len(X_train_all)):])\n",
        "\n",
        "# Calculate the mean absolute error\n",
        "train_mae = mean_absolute_error(Y_train_all, Y_train_pred)\n",
        "val_mae = mean_absolute_error(Y_train_all[-int(0.2 * len(Y_train_all)):], Y_val_pred)\n",
        "\n",
        "print(f\"Train MAE: {train_mae}, Validation MAE: {val_mae}\")\n",
        "\n",
        "all_test_data_real = pd.DataFrame()\n",
        "\n",
        "for location in locations:\n",
        "    test_data_loc = sort_data[sort_data['지점'] == location]\n",
        "    \n",
        "    # Drop the '지점' and '관측시점' columns\n",
        "    test_data_loc = test_data_loc.drop(['지점', '관측시점'], axis=1).values\n",
        "    \n",
        "    # Scale the data\n",
        "    scaler = scalers[location]\n",
        "    test_data_loc = scaler.transform(test_data_loc)\n",
        "\n",
        "    # Add one-hot encoded location to the test_data_loc\n",
        "    test_location_encoded = encoded_test_locations[sort_data['지점'] == location]\n",
        "    test_data_loc = np.hstack((test_location_encoded, test_data_loc))\n",
        "\n",
        "    # Prepare the data for LSTM\n",
        "    X_test, Y_test, missing_indices = prepare_data_with_missing(test_data_loc, lookback, future)\n",
        "\n",
        "    Y_test_missing_pred = model.predict(X_test)\n",
        "\n",
        "    for i, missing_index in enumerate(missing_indices):\n",
        "        start = missing_index + lookback\n",
        "        end = start + future\n",
        "        test_data_loc[start:end, 0] = Y_test_missing_pred[i]\n",
        "\n",
        "    # Inverse transform the test data to get real values\n",
        "    non_location_columns = test_data_loc[:, 17:]\n",
        "    test_data_loc_real = scaler.inverse_transform(non_location_columns)\n",
        "\n",
        "    # Create a new DataFrame with real values\n",
        "    test_data_loc_real_df = pd.DataFrame(test_data_loc_real, columns=['PM2.5', 'tem', 'wind degree', 'wind velocity', 'precipitation', 'humidity'])\n",
        "\n",
        "    # Get the '지점' and '관측시점' columns from the original test data\n",
        "    test_data_loc_observation = sort_data.loc[sort_data['지점'] == location, ['지점', '관측시점']].reset_index(drop=True)\n",
        "\n",
        "    # Concatenate '지점' and '관측시점' columns with the test_data_loc_real_df DataFrame\n",
        "    test_data_loc_real_df = pd.concat([test_data_loc_observation, test_data_loc_real_df], axis=1)\n",
        "\n",
        "    # Append the test_data_loc_real_df to the all_test_data_real DataFrame\n",
        "    all_test_data_real = all_test_data_real.append(test_data_loc_real_df, ignore_index=True)    \n",
        "\n",
        "# Sort the DataFrame by '지점' and '관측시점'\n",
        "all_test_data_real = all_test_data_real.sort_values(by=['지점', '관측시점']).reset_index(drop=True)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "all_test_data_real.to_csv('/content/new/result_47.csv', index=False)"
      ],
      "metadata": {
        "id": "kdZmZ-hEUwZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "one model and one hot incoding and hyper parameter"
      ],
      "metadata": {
        "id": "KiJzYuWMVMuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-tuner"
      ],
      "metadata": {
        "id": "7XSwMJd7UwcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import tensorflow as tf\n",
        "from kerastuner import HyperModel, RandomSearch\n",
        "\n",
        "np.random.seed(51)\n",
        "tf.random.set_seed(51)\n",
        "\n",
        "def has_missing_values(array):\n",
        "    return np.isnan(array).any()\n",
        "\n",
        "def prepare_data(data, lookback, future):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data) - lookback - future):\n",
        "        X.append(data[i : (i + lookback), :])\n",
        "        Y.append(data[(i + lookback) : (i + lookback + future), 17])  \n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "def prepare_data_with_missing(data, lookback, future):\n",
        "    X, Y, missing_indices = [], [], []\n",
        "    for i in range(len(data) - lookback - future):\n",
        "        input_sequence = data[i : (i + lookback), :]\n",
        "        output_sequence = data[(i + lookback) : (i + lookback + future), 17]\n",
        "\n",
        "        if has_missing_values(input_sequence):\n",
        "            continue\n",
        "\n",
        "        X.append(input_sequence)\n",
        "\n",
        "        if has_missing_values(output_sequence):\n",
        "            missing_indices.append(i)\n",
        "        else:\n",
        "            Y.append(output_sequence)\n",
        "\n",
        "    return np.array(X), np.array(Y), missing_indices\n",
        "\n",
        "class AirQualityLSTMHyperModel(HyperModel):\n",
        "    def __init__(self, input_shape, output_shape):\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = output_shape\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(\n",
        "            LSTM(\n",
        "                units=hp.Int(\"lstm_units\", min_value=32, max_value=512, step=32),\n",
        "                activation=\"tanh\",\n",
        "                input_shape=self.input_shape,\n",
        "                return_sequences=True,\n",
        "            )\n",
        "        )\n",
        "        model.add(Dropout(hp.Float(\"dropout_1\", min_value=0.1, max_value=0.5, step=0.1)))\n",
        "\n",
        "        model.add(\n",
        "            LSTM(\n",
        "                units=hp.Int(\"lstm_units\", min_value=32, max_value=512, step=32),\n",
        "                activation=\"tanh\",\n",
        "            )\n",
        "        )\n",
        "        model.add(Dropout(hp.Float(\"dropout_2\", min_value=0.1, max_value=0.5, step=0.1)))\n",
        "\n",
        "        model.add(Dense(self.output_shape))\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(\n",
        "                hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-2, sampling=\"LOG\")\n",
        "            ),\n",
        "            loss=\"mae\",\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "lookback = 2 * 24  # 2 days of hourly data\n",
        "future = 3 * 24    # 3 days of hourly data\n",
        "n_features = 17 + 6  # 17 one-hot encoded location columns and 6 other features in the data\n",
        "\n",
        "locations = ['공주', '노은동', '논산', '대천2동', '독곶리', '동문동', '모종동', '문창동', '성성동', '신방동', '신흥동', '아름동', '예산군', '읍내동', '이원면', '정림동', '홍성읍']  # List all 17 locations\n",
        "\n",
        "X_train_all = np.empty((0, lookback, n_features))\n",
        "Y_train_all = np.empty((0, future))\n",
        "scalers = {}\n",
        "\n",
        "locations = ['공주', '노은동', '논산', '대천2동', '독곶리', '동문동', '모종동', '문창동', '성성동', '신방동', '신흥동', '아름동', '예산군', '읍내동', '이원면', '정림동', '홍성읍']  # List all 17 locations\n",
        "\n",
        "X_train_all = np.empty((0, lookback, n_features))\n",
        "Y_train_all = np.empty((0, future))\n",
        "scalers = {}\n",
        "\n",
        "# Add EarlyStopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded_locations = encoder.fit_transform(sorted_data['지점'].values.reshape(-1, 1))\n",
        "encoded_test_locations = encoder.transform(sort_data['지점'].values.reshape(-1, 1))\n",
        "\n",
        "for location in locations:\n",
        "    train_data_loc = sorted_data[sorted_data['지점'] == location]\n",
        "    \n",
        "    # Drop the '지점' and '관측시점' columns\n",
        "    train_data_loc = train_data_loc.drop(['지점', '관측시점'], axis=1).values\n",
        "    \n",
        "    # Scale the data\n",
        "    scaler = MinMaxScaler()\n",
        "    train_data_loc[:, 0:] = scaler.fit_transform(train_data_loc[:, 0:])\n",
        "    scalers[location] = scaler\n",
        "\n",
        "    # Add one-hot encoded location to the train_data_loc\n",
        "    location_encoded = encoded_locations[sorted_data['지점'] == location]\n",
        "    train_data_loc = np.hstack((location_encoded, train_data_loc))\n",
        "\n",
        "    # Prepare the data for LSTM\n",
        "    X_train, Y_train = prepare_data(train_data_loc, lookback, future)\n",
        "    \n",
        "    X_train_all = np.vstack((X_train_all, X_train))\n",
        "    Y_train_all = np.vstack((Y_train_all, Y_train))\n",
        "\n",
        "# Define the hypermodel\n",
        "hypermodel = AirQualityLSTMHyperModel(input_shape=(lookback, n_features), output_shape=future)\n",
        "\n",
        "# Define the tuner\n",
        "tuner = RandomSearch(\n",
        "    hypermodel,\n",
        "    objective=\"val_loss\",\n",
        "    max_trials=20,\n",
        "    executions_per_trial=1,\n",
        "    directory=\"tuning\",\n",
        "    project_name=\"air_quality_lstm\",\n",
        ")\n",
        "\n",
        "# Set the early stopping callback\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
        "\n",
        "# Search for the best hyperparameters\n",
        "tuner.search(X_train_all, Y_train_all, epochs=100, verbose=0, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"Best Hyperparameters:\")\n",
        "print(f\"lstm_units: {best_hyperparameters.get('lstm_units')}\")\n",
        "print(f\"dropout_1: {best_hyperparameters.get('dropout_1')}\")\n",
        "print(f\"dropout_2: {best_hyperparameters.get('dropout_2')}\")\n",
        "print(f\"learning_rate: {best_hyperparameters.get('learning_rate')}\")\n",
        "\n",
        "# Build the best model\n",
        "best_model = tuner.hypermodel.build(best_hyperparameters)\n",
        "\n",
        "# Train the best model with validation split\n",
        "history = best_model.fit(X_train_all, Y_train_all, epochs=100, verbose=0, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Get the train and validation losses\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Make predictions\n",
        "Y_train_pred = model.predict(X_train_all)\n",
        "Y_val_pred = model.predict(X_train_all[-int(0.2 * len(X_train_all)):])\n",
        "\n",
        "# Calculate the mean absolute error\n",
        "train_mae = mean_absolute_error(Y_train_all, Y_train_pred)\n",
        "val_mae = mean_absolute_error(Y_train_all[-int(0.2 * len(Y_train_all)):], Y_val_pred)\n",
        "\n",
        "print(f\"Train MAE: {train_mae}, Validation MAE: {val_mae}\")\n",
        "\n",
        "all_test_data_real = pd.DataFrame()\n",
        "\n",
        "for location in locations:\n",
        "    test_data_loc = sort_data[sort_data['지점'] == location]\n",
        "    \n",
        "    # Drop the '지점' and '관측시점' columns\n",
        "    test_data_loc = test_data_loc.drop(['지점', '관측시점'], axis=1).values\n",
        "    \n",
        "    # Scale the data\n",
        "    scaler = scalers[location]\n",
        "    test_data_loc[:, 0:] = scaler.transform(test_data_loc[:, 0:])\n",
        "\n",
        "    # Add one-hot encoded location to the test_data_loc\n",
        "    test_location_encoded = encoded_test_locations[sort_data['지점'] == location]\n",
        "    test_data_loc = np.hstack((test_location_encoded, test_data_loc))\n",
        "\n",
        "    # Prepare the data for LSTM\n",
        "    X_test, Y_test, missing_indices = prepare_data_with_missing(test_data_loc, lookback, future)\n",
        "\n",
        "    Y_test_missing_pred = model.predict(X_test)\n",
        "\n",
        "    for i, missig_index in enumerate(missing_indices):\n",
        "        start = missig_index + lookback\n",
        "        end = start + future\n",
        "        test_data_loc[start:end, 17] = Y_test_missing_pred[i]\n",
        "\n",
        "    # Inverse transform the test data to get real values\n",
        "    non_location_columns = test_data_loc[:, 17:]\n",
        "    test_data_loc_real = scaler.inverse_transform(non_location_columns)\n",
        "\n",
        "    # Create a new DataFrame with real values\n",
        "    \n",
        "    test_data_loc_real_df = pd.DataFrame(test_data_loc_real, columns=['PM2.5', 'tem', 'wind degree', 'wind velocity', 'precipitation', 'humidity'])\n",
        "\n",
        "    # Get the '지점' and '관측시점' columns from the original test data\n",
        "    test_data_loc_observation = sort_data.loc[sort_data['지점'] == location, ['지점', '관측시점']].reset_index(drop=True)\n",
        "\n",
        "    # Concatenate '지점' and '관측시점' columns with the test_data_loc_real_df DataFrame\n",
        "    test_data_loc_real_df = pd.concat([test_data_loc_observation, test_data_loc_real_df], axis=1)\n",
        "\n",
        "    # Append the test_data_loc_real_df to the all_test_data_real DataFrame\n",
        "    all_test_data_real = all_test_data_real.append(test_data_loc_real_df, ignore_index=True) \n",
        "\n",
        "# Sort the DataFrame by '지점' and '관측시점'\n",
        "all_test_data_real = all_test_data_real.sort_values(by=['지점', '관측시점']).reset_index(drop=True)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "all_test_data_real.to_csv('/content/new/result_50.csv', index=False)"
      ],
      "metadata": {
        "id": "gCXAD1sUUwfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_test_data_real.head(100)\n",
        "\n",
        "result = all_test_data_real\n",
        "new_column = {'일시' : '관측시점'}\n",
        "result = result.rename(columns= new_column)\n",
        "result\n",
        "result.to_csv('/content/new/result_50.csv', index=False)\n",
        "result.head(50)"
      ],
      "metadata": {
        "id": "_8Gh9H3MUwii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/sample')\n",
        "all_files = glob.glob('*.csv')\n",
        "dfs = []\n",
        "for file in all_files:\n",
        "    df = pd.read_csv(file)\n",
        "    dfs.append(df)\n",
        "\n",
        "answer = pd.concat(dfs, ignore_index=True)\n",
        "new_column = {'측정소' : '지점'}\n",
        "answer = answer.rename(columns= new_column)\n",
        "answer['관측시점'] = answer['연도'].apply(convert_year) + '-' + answer['일시']\n",
        "answer"
      ],
      "metadata": {
        "id": "kVrkDLm0Uwme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the result DataFrame with the answer sheet\n",
        "final_data = answer.merge(result[['관측시점', '지점', 'PM2.5']], on=['관측시점', '지점'], how='left')\n",
        "final_data.drop(['PM2.5_x', '관측시점'], axis=1, inplace=True)\n",
        "final_data.rename(columns={'PM2.5_y': 'PM2.5'}, inplace=True)\n",
        "final_data.rename(columns={'지점': '측정소'}, inplace=True)\n",
        "final_data\n",
        "\n",
        "final_data.to_csv('/content/new/ans_50.csv', index=False)"
      ],
      "metadata": {
        "id": "us643-_sUwu_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}